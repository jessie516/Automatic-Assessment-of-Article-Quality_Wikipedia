{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8yCTjrG-XAJ",
        "outputId": "1063b9cd-bcf1-451e-9aff-99c0e8da4509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OICNZ_rH-f42",
        "outputId": "993b9f69-6548-4bc5-97b0-548fe01b44af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mwxml\n",
            "  Downloading mwxml-0.3.3-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from mwxml) (4.19.2)\n",
            "Collecting mwcli>=0.0.2 (from mwxml)\n",
            "  Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting mwtypes>=0.3.0 (from mwxml)\n",
            "  Downloading mwtypes-0.3.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting para>=0.0.1 (from mwxml)\n",
            "  Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->mwxml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->mwxml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->mwxml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->mwxml) (0.18.1)\n",
            "Collecting docopt (from mwcli>=0.0.2->mwxml)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonable>=0.3.0 (from mwtypes>=0.3.0->mwxml)\n",
            "  Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=ab6898b147fbda2099c7d6fb68611f84d83e4a0477bdb85c320a1853097edbc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: para, jsonable, docopt, mwtypes, mwcli, mwxml\n",
            "Successfully installed docopt-0.6.2 jsonable-0.3.1 mwcli-0.0.3 mwtypes-0.3.2 mwxml-0.3.3 para-0.0.8\n"
          ]
        }
      ],
      "source": [
        "! pip install mwxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWO90moHaYN_",
        "outputId": "0b2d0f41-4198-4573-ff1a-8e35cd06a7c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import bz2\n",
        "import mwxml\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from datetime import datetime, timedelta\n",
        "#from mwxml import Timestamp\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esoNDxi1xJJa"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "bz2_file = '/content/drive/MyDrive/학교/Dissertation/enwiki-20240601-pages-articles-multistream.xml.bz2'\n",
        "xml_file = '/content/drive/MyDrive/학교/Dissertation/enwiki-20240601-pages-articles-multistream.xml'\n",
        "csv_file = '/content/drive/MyDrive/학교/Dissertation/enwiki_quality_ratings.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lpey-5k4MZ5_"
      },
      "outputs": [],
      "source": [
        "# Decompress the bz2 file\n",
        "with bz2.open(bz2_file, 'rb') as f_in, open(xml_file, 'wb') as f_out:\n",
        "    for data in iter(lambda: f_in.read(100 * 1024), b''):\n",
        "        f_out.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KHwO68k_fn5"
      },
      "outputs": [],
      "source": [
        "# Function to determine article quality\n",
        "def determine_quality(text):\n",
        "    quality_templates = {\n",
        "        'FA': r'\\{\\{Featured[ _]article',\n",
        "        'GA': r'\\{\\{Good[ _]article',\n",
        "        'B': r'\\{\\{B[ _]class',\n",
        "        'C': r'\\{\\{C[ _]class',\n",
        "        'Start': r'\\{\\{Start[ _]class',\n",
        "        'Stub': r'\\{\\{Stub[ _]class'\n",
        "    }\n",
        "    for quality, pattern in quality_templates.items():\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            return quality\n",
        "    return 'Unknown'\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiouy\"\n",
        "    if word[0] in vowels:\n",
        "        count = 1\n",
        "    else:\n",
        "        count = 0\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count = 1\n",
        "    return count\n",
        "\n",
        "# Function to identify complex words\n",
        "def is_complex_word(word):\n",
        "    syllables = count_syllables(word)\n",
        "    # Define criteria for complex words, e.g., more than two syllables\n",
        "    return syllables > 2\n",
        "\n",
        "# Function to calculate section sizes\n",
        "def calculate_section_sizes(text):\n",
        "    section_sizes = []\n",
        "    sections = re.split(r'==[^=].*==', text)\n",
        "    for section in sections:\n",
        "        section_size = len(section.split())\n",
        "        section_sizes.append(section_size)\n",
        "    if section_sizes:\n",
        "        longest_section = max(section_sizes)\n",
        "        shortest_section = min(section_sizes)\n",
        "        mean_section_size = sum(section_sizes) / len(section_sizes)\n",
        "    else:\n",
        "        longest_section = shortest_section = mean_section_size = 0\n",
        "    return longest_section, shortest_section, mean_section_size\n",
        "\n",
        "# Function to count external links\n",
        "def count_external_links(text):\n",
        "    external_links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
        "    return len(external_links)\n",
        "\n",
        "# Function to count internal links\n",
        "def count_internal_links(text):\n",
        "    internal_links = re.findall(r'\\[\\[([^\\]|]+)(?:\\|([^\\]]+))?\\]\\]', text)\n",
        "    return len(internal_links)\n",
        "\n",
        "# Function to count images\n",
        "def count_images(text):\n",
        "    image_count = len(re.findall(r'\\[\\[File:[^\\]]*\\]\\]', text)) + len(re.findall(r'\\[\\[Image:[^\\]]*\\]\\]', text))\n",
        "    return image_count\n",
        "\n",
        "# Function to count questions\n",
        "def count_questions(text):\n",
        "    return len(re.findall(r'\\?', text))\n",
        "\n",
        "# Function to count exclamations\n",
        "def count_exclamations(text):\n",
        "    return len(re.findall(r'\\!', text))\n",
        "\n",
        "# Function to count sentences starting with a pronoun\n",
        "def count_sentences_starting_with_pronoun(sentences):\n",
        "    pronoun_tags = {'PRP', 'PRP$', 'WP', 'WP$'}\n",
        "    count = 0\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        if words:\n",
        "            first_word = pos_tag([words[0]])[0]\n",
        "            if first_word[1] in pronoun_tags:\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "# Function to calculate revert count\n",
        "def calculate_revert_count(page):\n",
        "    revision_count = len(page.revisions)\n",
        "    revert_count = 0\n",
        "    for i in range(1, revision_count):\n",
        "        if page.revisions[i].parent_id != page.revisions[i - 1].id:\n",
        "            revert_count += 1\n",
        "    return revert_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F543JcdxABe1"
      },
      "outputs": [],
      "source": [
        "# Function to process each page and extract data\n",
        "def process_page(page):\n",
        "    title = page.title\n",
        "    ns = page.namespace\n",
        "    page_id = page.id\n",
        "\n",
        "    # Extract the latest revision info\n",
        "    if page.revisions:\n",
        "        latest_revision = page.revisions[0]\n",
        "        creation_date = page.revisions[-1].timestamp\n",
        "        revision_id = latest_revision.id\n",
        "        timestamp = latest_revision.timestamp\n",
        "        contributor = latest_revision.contributor\n",
        "        contributor_name = contributor.username if contributor.username else contributor.ip\n",
        "        comment = latest_revision.comment\n",
        "        text = latest_revision.text or \"\"\n",
        "\n",
        "        # text\n",
        "        text = latest_revision.text or \"\"\n",
        "        character_count = len(text)\n",
        "        words = word_tokenize(text)\n",
        "        word_count = len(words)\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentence_count = len(sentences)\n",
        "        syllable_count = sum(count_syllables(word) for word in words)\n",
        "        complex_word_count = sum(1 for word in words if is_complex_word(word))\n",
        "\n",
        "        # structure\n",
        "        section_count = len(re.findall(r'==[^=].*==', text))\n",
        "        subsection_count = len(re.findall(r'===.*===', text))\n",
        "        paragraphs = re.split(r'\\n\\n+', text)\n",
        "        paragraph_count = len(paragraphs)\n",
        "        mean_paragraph_size = sum(len(paragraph.split()) for paragraph in paragraphs) / paragraph_count if paragraph_count > 0 else 0\n",
        "        longest_section, shortest_section, mean_section_size = calculate_section_sizes(text)\n",
        "        longest_shortest_ratio = longest_section / shortest_section if shortest_section > 0 else float('inf')\n",
        "        citation_count = len(re.findall(r'<ref[^>]*>', text))\n",
        "        external_link_count = count_external_links(text)\n",
        "        internal_link_count = count_internal_links(text)\n",
        "        links_per_text_length = (external_link_count + internal_link_count) / character_count if character_count > 0 else 0\n",
        "        image_count = count_images(text)\n",
        "        images_per_text_length = image_count / character_count if character_count > 0 else 0\n",
        "\n",
        "        # style\n",
        "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
        "        mean_sentence_size = sum(sentence_lengths) / sentence_count if sentence_count > 0 else 0\n",
        "        largest_sentence_size = max(sentence_lengths) if sentence_lengths else 0\n",
        "        shortest_sentence_size = min(sentence_lengths) if sentence_lengths else 0\n",
        "        question_count = count_questions(text)\n",
        "        question_ratio = question_count / sentence_count if sentence_count > 0 else 0\n",
        "        exclamation_count = count_exclamations(text)\n",
        "        exclamation_ratio = exclamation_count / sentence_count if sentence_count > 0 else 0\n",
        "        sentences_starting_with_pronoun_count = count_sentences_starting_with_pronoun(sentences)\n",
        "\n",
        "        # review\n",
        "        current_date = datetime.utcnow()\n",
        "        article_age_days = (current_date - creation_date).days\n",
        "        revision_dates = [rev.timestamp for rev in page.revisions]\n",
        "        revision_intervals = [(revision_dates[i] - revision_dates[i - 1]).days for i in range(1, len(revision_dates))]\n",
        "        mean_revision_age = sum(revision_intervals) / len(revision_intervals) if revision_intervals else 0\n",
        "        review_count = len(page.revisions)\n",
        "        reviews_per_day = review_count / article_age_days if article_age_days > 0 else 0\n",
        "        contributors = [rev.contributor.username if rev.contributor.username else rev.contributor.ip for rev in page.revisions]\n",
        "        unique_contributors = set(contributors)\n",
        "        user_count = len(unique_contributors)\n",
        "        reviews_per_user = review_count / user_count if user_count > 0 else 0\n",
        "        registered_users = [user for user in unique_contributors if re.match(r'[a-zA-Z0-9_]+', user)]\n",
        "        anonymous_users = [user for user in unique_contributors if re.match(r'(\\d{1,3}\\.){3}\\d{1,3}', user)]\n",
        "        registered_user_count = len(registered_users)\n",
        "        anonymous_user_count = len(anonymous_users)\n",
        "        contributor_counts = {user: contributors.count(user) for user in unique_contributors}\n",
        "        occasional_user_count = sum(1 for count in contributor_counts.values() if count <= 2)\n",
        "        diversity = user_count / review_count if review_count > 0 else 0\n",
        "        discussion_count = sum(1 for rev in page.revisions if rev.comment and 'talk' in rev.comment.lower())\n",
        "        revert_count = calculate_revert_count(page)\n",
        "\n",
        "        # quality\n",
        "        quality = determine_quality(text)\n",
        "        return {\n",
        "            'title': title,\n",
        "            'namespace': ns,\n",
        "            'page_id': page_id,\n",
        "            'revision_id': revision_id,\n",
        "            'timestamp': timestamp,\n",
        "            'contributor': contributor_name,\n",
        "            'comment': comment,\n",
        "\n",
        "            'character_count': character_count,\n",
        "            'word_count': word_count,\n",
        "            'sentence_count': sentence_count,\n",
        "            'syllable_count': syllable_count,\n",
        "            'complex_word_count': complex_word_count,\n",
        "\n",
        "            'section_count': section_count,\n",
        "            'subsection_count': subsection_count,\n",
        "            'paragraph_count': paragraph_count,\n",
        "            'mean_section_size': mean_section_size,\n",
        "            'mean_paragraph_size': mean_paragraph_size,\n",
        "            'longest_section_size': longest_section,\n",
        "            'shortest_section_size': shortest_section,\n",
        "            'longest_shortest_ratio': longest_section,\n",
        "            'citation_count': citation_count,\n",
        "            'external_link_count': external_link_count,\n",
        "            'internal_link_count': internal_link_count,\n",
        "            'links_per_text_length': links_per_text_length,\n",
        "            'image_count': image_count,\n",
        "            'images_per_text_length': images_per_text_length,\n",
        "\n",
        "            'mean_sentence_size': mean_sentence_size,\n",
        "            'largest_sentence_size': largest_sentence_size,\n",
        "            'shortest_sentence_size': shortest_sentence_size,\n",
        "            'question_count': question_count,\n",
        "            'question ratio': question_ratio,\n",
        "            'exclamation_count': exclamation_count,\n",
        "            'exclamation_ratio': exclamation_ratio,\n",
        "            'sentences_starting_with_pronount': sentences_starting_with_pronoun_count,\n",
        "\n",
        "            'article_age_days': article_age_days,\n",
        "            'mean_revision_age': mean_revision_age,\n",
        "            'review_count': review_count,\n",
        "            'reviews_per_day': reviews_per_day,\n",
        "            'user_count': user_count,\n",
        "            'reviews_per_user': reviews_per_user,\n",
        "            'registered_user_count': registered_user_count,\n",
        "            'anonymous_user_count': anonymous_user_count,\n",
        "            'occasional_user_count': occasional_user_count,\n",
        "            'diversity': diversity,\n",
        "            'discussion_count': discussion_count,\n",
        "            'revert_count': revert_count,\n",
        "\n",
        "            'quality': quality\n",
        "        }\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "AWTCrpabxYsr",
        "outputId": "bb6fd4ad-ab18-4aa0-fa78-c268213b0864"
      },
      "outputs": [
        {
          "ename": "ParseError",
          "evalue": "no element found: line 1, column 0: b''... (<string>)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/mwxml/element_iterator.py\"\u001b[0m, line \u001b[1;32m100\u001b[0m, in \u001b[1;35mfrom_file\u001b[0m\n    event, element = next(pointer)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/mwxml/element_iterator.py\"\u001b[0m, line \u001b[1;32m22\u001b[0m, in \u001b[1;35m__next__\u001b[0m\n    event, element = next(self.etree_events)\n",
            "  File \u001b[1;32m\"/usr/lib/python3.10/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1259\u001b[0m, in \u001b[1;35miterator\u001b[0m\n    root = pullparser._close_and_return_root()\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/lib/python3.10/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m1302\u001b[0;36m, in \u001b[0;35m_close_and_return_root\u001b[0;36m\u001b[0m\n\u001b[0;31m    root = self._parser.close()\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m no element found: line 1, column 0\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-7-811a663baa5e>\"\u001b[0m, line \u001b[1;32m4\u001b[0m, in \u001b[1;35m<cell line: 4>\u001b[0m\n    with mwxml.Dump.from_file(open(xml_file, 'rb')) as dump:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/mwxml/iteration/dump.py\"\u001b[0m, line \u001b[1;32m144\u001b[0m, in \u001b[1;35mfrom_file\u001b[0m\n    element = ElementIterator.from_file(f)\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.10/dist-packages/mwxml/element_iterator.py\"\u001b[0;36m, line \u001b[0;32m103\u001b[0;36m, in \u001b[0;35mfrom_file\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise ParseError(\"{0}: {1}...\"\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m no element found: line 1, column 0: b''...\n"
          ]
        }
      ],
      "source": [
        "# Parse the XML dump and extract data\n",
        "data = []\n",
        "\n",
        "with mwxml.Dump.from_file(open(xml_file, 'rb')) as dump:\n",
        "    for page in tqdm(dump, desc=\"Processing pages\", unit=\" pages\"):\n",
        "        if page.namespace == 0:  # Only consider articles in the main namespace\n",
        "            page_data = process_page(page)\n",
        "            if page_data:\n",
        "                data.append(page_data)\n",
        "\n",
        "# Ensure all data entries are dictionaries\n",
        "data2 = [d for d in data if isinstance(d, dict)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4rjLDrDAMqG"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame and save to CSV\n",
        "df = pd.DataFrame(data2)\n",
        "df.to_csv(csv_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ8bdUfrAUOu"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}